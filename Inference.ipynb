{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBrEbVqzYw0D",
        "outputId": "68842a1e-c5f3-4d8f-9da7-6d8339702de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "OUTFIT MATCHER - Re-PolyVore Dataset\n",
            "============================================================\n",
            "\n",
            "Attempt 1: Loading 'siameese_embedding.h5'...\n",
            "✓ Successfully loaded siameese_embedding.h5\n",
            "\n",
            "Available categories:\n",
            " 1. bag\n",
            " 2. bracelet\n",
            " 3. brooch\n",
            " 4. dress\n",
            " 5. earrings\n",
            " 6. eyewear\n",
            " 7. gloves\n",
            " 8. hairwear\n",
            " 9. hats\n",
            "10. jumpsuit\n",
            "11. legwear\n",
            "12. necklace\n",
            "13. neckwear\n",
            "14. outwear\n",
            "15. pants\n",
            "16. rings\n",
            "17. shoes\n",
            "18. skirt\n",
            "19. top\n",
            "20. watches\n",
            "\n",
            "Choose wardrobe loading option:\n",
            "1. Load specific categories\n",
            "2. Load all categories\n",
            "3. Load common categories (top, pants, shoes, bag, dress)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications import resnet\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"OUTFIT MATCHER - Re-PolyVore Dataset\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Try loading the model with error handling\n",
        "def load_embedding_model():\n",
        "    \"\"\"Load embedding model with multiple fallback options\"\"\"\n",
        "\n",
        "    # Option 1: Try loading siameese_embedding.h5\n",
        "    try:\n",
        "        print(\"\\nAttempt 1: Loading 'siameese_embedding.h5'...\")\n",
        "        model = load_model('siameese_embedding.h5', compile=False)\n",
        "        print(\" Successfully loaded siameese_embedding.h5\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\" Failed: {e}\")\n",
        "\n",
        "    # Option 2: Try loading siamese_model.h5 and extract embedding\n",
        "    try:\n",
        "        print(\"\\nAttempt 2: Loading 'siamese_model.h5' and extracting embedding...\")\n",
        "        full_model = load_model('siamese_model.h5', compile=False,\n",
        "                               custom_objects={'DistanceLayer': DistanceLayer})\n",
        "\n",
        "        # Extract the embedding model from the siamese network\n",
        "        # The embedding is the first layer of the siamese network\n",
        "        embedding_model = full_model.layers[3]  # Adjust index if needed\n",
        "        print(\" Successfully extracted embedding from siamese_model.h5\")\n",
        "        return embedding_model\n",
        "    except Exception as e:\n",
        "        print(\" Failed: {e}\")\n",
        "\n",
        "    # Option 3: Try loading with keras\n",
        "    try:\n",
        "        print(\"\\nAttempt 3: Loading with tf.keras...\")\n",
        "        model = tf.keras.models.load_model('siameese_embedding.h5', compile=False)\n",
        "        print(\" Successfully loaded with tf.keras\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\" Failed: {e}\")\n",
        "\n",
        "    # Option 4: Rebuild the model architecture\n",
        "    try:\n",
        "        print(\"\\nAttempt 4: Rebuilding model architecture and loading weights...\")\n",
        "        from tensorflow.keras import layers, Model\n",
        "\n",
        "        # Rebuild embedding model architecture (same as training)\n",
        "        base_cnn = resnet.ResNet50(\n",
        "            weights=\"imagenet\",\n",
        "            input_shape=(200, 200, 3),\n",
        "            include_top=False\n",
        "        )\n",
        "\n",
        "        flatten = layers.Flatten()(base_cnn.output)\n",
        "        dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
        "        dense1 = layers.BatchNormalization()(dense1)\n",
        "        dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n",
        "        dense2 = layers.BatchNormalization()(dense2)\n",
        "        output = layers.Dense(256)(dense2)\n",
        "\n",
        "        embedding = Model(base_cnn.input, output, name=\"Embedding\")\n",
        "\n",
        "        # Try to load weights\n",
        "        embedding.load_weights('siameese_embedding.h5')\n",
        "        print(\"Successfully rebuilt model and loaded weights\")\n",
        "        return embedding\n",
        "    except Exception as e:\n",
        "        print(f\" Failed: {e}\")\n",
        "\n",
        "    raise Exception(\" All loading attempts failed. Please check your model files.\")\n",
        "\n",
        "# Custom layer needed for siamese_model.h5\n",
        "class DistanceLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, anchor, positive, negative):\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
        "        return (ap_distance, an_distance)\n",
        "\n",
        "# Load the model\n",
        "embedding_model = load_embedding_model()\n",
        "\n",
        "target_shape = (200, 200)\n",
        "\n",
        "def preprocess_image(filename):\n",
        "    \"\"\"Load and preprocess image\"\"\"\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, target_shape)\n",
        "    return image\n",
        "\n",
        "def get_embedding(image_path):\n",
        "    \"\"\"Get embedding vector for an image\"\"\"\n",
        "    image = preprocess_image(image_path)\n",
        "    image_batch = np.expand_dims(image, axis=0)\n",
        "    image_batch = np.array(image_batch, dtype=np.float32, copy=True)\n",
        "\n",
        "    # Preprocess for ResNet\n",
        "    preprocessed = resnet.preprocess_input(image_batch)\n",
        "\n",
        "    # Get embedding\n",
        "    embedding = embedding_model.predict(preprocessed, verbose=0)\n",
        "    return embedding[0]\n",
        "\n",
        "def compute_cosine_similarity(emb1, emb2):\n",
        "    \"\"\"Compute cosine similarity between two embeddings\"\"\"\n",
        "    dot_product = np.dot(emb1, emb2)\n",
        "    norm1 = np.linalg.norm(emb1)\n",
        "    norm2 = np.linalg.norm(emb2)\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "def load_wardrobe(base_path, categories, limit_per_category=None):\n",
        "    \"\"\"Load clothing items from specified categories\"\"\"\n",
        "    wardrobe = {}\n",
        "\n",
        "    print(f\"\\nLoading wardrobe from '{base_path}'...\")\n",
        "\n",
        "    for category in categories:\n",
        "        category_path = os.path.join(base_path, category)\n",
        "\n",
        "        if not os.path.exists(category_path):\n",
        "            print(f\"⚠ Category '{category}' not found at {category_path}\")\n",
        "            continue\n",
        "\n",
        "        # Get all image files\n",
        "        image_files = []\n",
        "        for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
        "            image_files.extend(Path(category_path).glob(ext))\n",
        "\n",
        "        # Limit if specified\n",
        "        if limit_per_category and len(image_files) > limit_per_category:\n",
        "            image_files = image_files[:limit_per_category]\n",
        "\n",
        "        wardrobe[category] = [str(f) for f in image_files]\n",
        "        print(f\" Loaded {len(wardrobe[category])} items from '{category}'\")\n",
        "\n",
        "    return wardrobe\n",
        "\n",
        "def find_best_match(query_path, category_items, top_k=5):\n",
        "    \"\"\"Find top-k best matching items from a category\"\"\"\n",
        "    query_emb = get_embedding(query_path)\n",
        "\n",
        "    results = []\n",
        "    total = len(category_items)\n",
        "\n",
        "    print(f\"Processing {total} items...\")\n",
        "    for idx, item_path in enumerate(category_items):\n",
        "        if (idx + 1) % 500 == 0:\n",
        "            print(f\"  Processed {idx + 1}/{total} items...\")\n",
        "\n",
        "        try:\n",
        "            item_emb = get_embedding(item_path)\n",
        "            similarity = compute_cosine_similarity(query_emb, item_emb)\n",
        "            results.append({'path': item_path, 'similarity': similarity})\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    # Sort by similarity\n",
        "    results.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "    return results[:top_k]\n",
        "\n",
        "def visualize_matches(query_path, matches, title=\"Outfit Matches\"):\n",
        "    \"\"\"Visualize query image with its matches\"\"\"\n",
        "    n_matches = len(matches)\n",
        "    fig, axes = plt.subplots(1, n_matches + 1, figsize=(3 * (n_matches + 1), 3))\n",
        "\n",
        "    # Query image\n",
        "    query_img = preprocess_image(query_path)\n",
        "    if n_matches == 0:\n",
        "        axes.imshow(query_img)\n",
        "        axes.set_title(\"Your Item\", fontweight='bold')\n",
        "        axes.axis('off')\n",
        "    else:\n",
        "        axes[0].imshow(query_img)\n",
        "        axes[0].set_title(\"Your Item\", fontweight='bold')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        # Matches\n",
        "        for i, match in enumerate(matches):\n",
        "            match_img = preprocess_image(match['path'])\n",
        "            axes[i + 1].imshow(match_img)\n",
        "            axes[i + 1].set_title(f\"Match {i+1}\\n{match['similarity']:.3f}\")\n",
        "            axes[i + 1].axis('off')\n",
        "\n",
        "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main interactive function\"\"\"\n",
        "\n",
        "    # Get available categories\n",
        "    base_path = 'Re-PolyVore'\n",
        "\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"\\n Error: '{base_path}' folder not found!\")\n",
        "        print(\"Please make sure Re-PolyVore dataset is in the current directory.\")\n",
        "        return\n",
        "\n",
        "    available_categories = sorted([d for d in os.listdir(base_path)\n",
        "                                  if os.path.isdir(os.path.join(base_path, d))])\n",
        "\n",
        "    print(f\"\\nAvailable categories:\")\n",
        "    for i, cat in enumerate(available_categories, 1):\n",
        "        print(f\"{i:2d}. {cat}\")\n",
        "\n",
        "    # Category selection\n",
        "    print(\"\\nChoose wardrobe loading option:\")\n",
        "    print(\"1. Load specific categories\")\n",
        "    print(\"2. Load all categories\")\n",
        "    print(\"3. Load common categories (top, pants, shoes, bag, dress)\")\n",
        "\n",
        "    choice = input(\"\\nEnter your choice (1/2/3): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        cat_input = input(\"Enter category names (comma-separated, e.g., top,pants,shoes): \")\n",
        "        selected_categories = [c.strip() for c in cat_input.split(',')]\n",
        "    elif choice == '2':\n",
        "        selected_categories = available_categories\n",
        "    else:\n",
        "        selected_categories = ['top', 'pants', 'shoes', 'bag', 'dress']\n",
        "\n",
        "    # Filter valid categories\n",
        "    selected_categories = [c for c in selected_categories if c in available_categories]\n",
        "\n",
        "    # Check dataset size\n",
        "    total_items = 0\n",
        "    for cat in selected_categories:\n",
        "        cat_path = os.path.join(base_path, cat)\n",
        "        num_items = len(list(Path(cat_path).glob('*.jpg')))\n",
        "        total_items += num_items\n",
        "\n",
        "    limit_per_category = None\n",
        "    if total_items > 5000:\n",
        "        print(f\"\\n Large dataset detected ({total_items} total images)\")\n",
        "        print(\"Options:\")\n",
        "        print(\"1. Load ALL images (thorough but slower)\")\n",
        "        print(\"2. Load limited images (faster, e.g., 100, 500, 1000)\")\n",
        "\n",
        "        limit_choice = input(\"\\nLoad all images? (1=all, 2=limited): \").strip()\n",
        "        if limit_choice == '2':\n",
        "            limit_per_category = int(input(\"Enter max items per category (e.g., 500): \").strip())\n",
        "        else:\n",
        "            print(\"Loading ALL images from each category...\")\n",
        "\n",
        "    # Load wardrobe\n",
        "    wardrobe = load_wardrobe(base_path, selected_categories, limit_per_category)\n",
        "    print(f\"\\n✓ Wardrobe loaded with {len(wardrobe)} categories\")\n",
        "\n",
        "    # Get query image\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    query_path = input(\"Enter path to your clothing item (or press Enter for sample): \").strip()\n",
        "\n",
        "    if not query_path or not os.path.exists(query_path):\n",
        "        # Use a sample from the first category\n",
        "        first_cat = list(wardrobe.keys())[0]\n",
        "        query_path = wardrobe[first_cat][0]\n",
        "        print(f\"Using sample: {query_path}\")\n",
        "\n",
        "    # Matching options\n",
        "    print(f\"\\nCategories in your wardrobe:\")\n",
        "    for i, (cat, items) in enumerate(wardrobe.items(), 1):\n",
        "        print(f\"{i}. {cat} ({len(items)} items)\")\n",
        "\n",
        "    print(\"\\nMatching Options:\")\n",
        "    print(\"1. Match with specific category (see top 5 matches)\")\n",
        "    print(\"2. Get complete outfit (best match from each category)\")\n",
        "\n",
        "    match_choice = input(\"\\nEnter your choice (1 or 2): \").strip()\n",
        "\n",
        "    if match_choice == '1':\n",
        "        # Match with specific category\n",
        "        target_cat = input(\"Enter category to match with: \").strip()\n",
        "\n",
        "        if target_cat not in wardrobe:\n",
        "            print(f\" Category '{target_cat}' not loaded in wardrobe\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nSearching through {len(wardrobe[target_cat])} {target_cat} items...\")\n",
        "        matches = find_best_match(query_path, wardrobe[target_cat], top_k=5)\n",
        "\n",
        "        print(f\"\\n Found {len(matches)} matches\")\n",
        "        for i, match in enumerate(matches, 1):\n",
        "            print(f\"{i}. {os.path.basename(match['path'])} - Similarity: {match['similarity']:.4f}\")\n",
        "\n",
        "        visualize_matches(query_path, matches, title=f\"Best {target_cat} matches\")\n",
        "\n",
        "    else:\n",
        "        # Complete outfit\n",
        "        print(f\"\\nFinding best matches from: {', '.join(wardrobe.keys())}\")\n",
        "\n",
        "        all_matches = {}\n",
        "        for category, items in wardrobe.items():\n",
        "            print(f\"\\nSearching {category}...\")\n",
        "            matches = find_best_match(query_path, items, top_k=1)\n",
        "            if matches:\n",
        "                all_matches[category] = matches[0]\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"COMPLETE OUTFIT\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        for category, match in all_matches.items():\n",
        "            print(f\"{category.upper()}: {os.path.basename(match['path'])}\")\n",
        "            print(f\"  Similarity: {match['similarity']:.4f}\\n\")\n",
        "\n",
        "        # Visualize each category match\n",
        "        for category, match in all_matches.items():\n",
        "            visualize_matches(query_path, [match], title=f\"Best {category} match\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVMTtJzOZDDC",
        "outputId": "fa7756f3-4c18-4f55-a3e7-8467c80c5f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction completed!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/Colab Notebooks/Re-PolyVore.zip'\n",
        "extract_path = '/content'\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}